---
# title: "Untitled"
output: html_document
date: "2025-11-03"
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = '~/Downloads')
```

```{r}
library(tibble)
library(terra)
library(dplyr)
library(comprehenr)
library(lubridate)

setwd('~/Downloads')

end_year = 2024
amo = read.table("hurricane_project/amon.us.long.data.txt",strip.white = TRUE)
months = as.vector(amo[1,])
colnames(amo) = months
amo = amo[-1,]
amo = amo[amo$Year>=1960,]
amo = amo[amo$Year<=end_year,]
amo = amo %>% mutate(amo=(as.numeric(May)+as.numeric(Jun))/2 )

soi = read.table("hurricane_project/soi.txt",strip.white = TRUE)
colnames(soi) = months
soi = soi[soi$Year>=1960,]
soi = soi[soi$Year<=end_year,]
soi = soi %>% mutate(soi=(as.numeric(May)+as.numeric(Jun))/2 )

nao = read.table("hurricane_project/nao.data.txt",strip.white = TRUE)
colnames(nao) = months
nao[nao$Year>=1960,]
nao = nao[nao$Year>=1960,] 
nao = nao[nao$Year<=end_year,]
nao = nao %>% mutate(nao=(as.numeric(May)+as.numeric(Jun))/2 )

nino = read.table("hurricane_project/nino.ascii",strip.white = TRUE)
nino = nino[-1,c(1,2,9)]
foo = matrix(NA, nrow=76, ncol=12)
count=1
for (i in 1:76){
  for (j in 1:12){
    foo[i,j]=nino[count,3]
    count = count+1
    if(count==911) break
  }
}
foo = data.frame(foo)
nino = add_column(foo, Year = 1950:2025,.before="X1")
colnames(nino)=months
nino = nino[nino$Year>=1960,]
nino = nino[nino$Year<=end_year,]
nino = nino %>% mutate(nino=(as.numeric(May)+as.numeric(Jun))/2 )
```
```{}
tmpdir <- tempdir()
untar("hurricane_project/sst.tar.gz", exdir = tmpdir)

nc_files <- list.files(tmpdir, pattern = "\\.nc$", full.names = TRUE, recursive=TRUE)
nc_files

```
```{}

file <- nc_files[grepl("/tmp/RtmpXueFnm/data/netcdf3.0.2new/degree1/enh/sst.mean.nc", nc_files)]

r <- rast(file)
r

```
```{}
df1 <- as.data.frame(r, xy = TRUE)
head(df1)

```
```{}
write.csv(df1,"hurricane_project/sst_raw.csv", row.names = FALSE)
```
```{r}
setwd('~/Downloads')
list.files()
df1 = read.csv("hurricane_project/sst_raw.csv")
df1 = filter(df1, x >= 280  & x <= 340)
df1 = filter(df1, y >= 10  & y <= 25)
```
```{r}
month = seq(as.Date("1960-01-01"), as.Date("2025-07-01"), by="month")
sst_mean = to_vec(for(i in 3:789) mean(df1[,i],na.rm=T))

sst_month = data.frame(date=month, sst_mean=sst_mean)
sst_month = mutate(sst_month, year=year(date))

may_jun = to_vec(for(i in 1:66) (sst_month[12*(i-1)+5,2] + sst_month[12*(i-1)+6,2])/2 )
sst = data.frame(Year = 1960:2025, sst=may_jun)
sst = sst[sst$Year<=end_year,]
```


```{r}
setwd('~/Downloads')
loss = read.csv("hurricane_project/08302025_2024_aggregate.csv")
met = read.csv("hurricane_project/lf_matched_metrics-1.csv")
hurdat = read.csv("hurricane_project/hurdat_track-1761700598833-1.csv")

df = merge(loss,met,by=c("storm_year","storm_name","storm_basin"))
df = df[df$storm_year >= 1960,]
df = df[,-3]

hurdat = hurdat[hurdat$storm_year >= 1960,]

# removing multiple landfalls, not relevant for this model
df = filter(df, lf_id == 1)
d = duplicated(df$hurdatId)
df = df[!d,]
```

```{r}
hurdat2 = filter(hurdat, !(storm_status %in% c('TD','DB','LO','WV','SD')))
```

```{r}
name=''
for (i in 1:nrow(hurdat2)){
  if (hurdat2[i,]$storm_name != name) name = hurdat2[i,]$storm_name
  else hurdat2[i,]$storm_name=NA
}
```

```{r}
dupe = is.na(hurdat2$storm_name)
hurdat2 = hurdat2[!dupe,]
```
```{r}
hurdat2 = mutate(hurdat2, ID=paste(as.character(storm_year),storm_name))
df = mutate(df, ID=paste(as.character(storm_year),storm_name))
```
```{r}
season = merge(hurdat2,df,by='ID',all=T)
season = season[,c(3,4,5,30,31)]
colnames(season) = c('storm_year','storm_name','datetime','mmh24','mmp24')
season = season %>% mutate(storm_year_alt = ifelse(month(datetime)==1,storm_year-1,storm_year),mmp24 = ifelse(is.na(mmp24),0,mmp24),mmh24 = ifelse(is.na(mmh24),0,mmh24))

```
```{r}
# dropped amo because data only until 2022, fix later
# data prep

N_obs = rep(NA,2025-1960)
count=0
year = 1960
for (i in season$storm_year_alt){
  if(i == year) count = count+1
  else{
    year = i
    N_obs[year-1960] = count
    count = 1
  }
}
N_obs[65] = count

row=1
count=0
L_obs = rep(NA,2025-1960)
for (i in 1:65){
  for (j in 1:N_obs[i]){
    if (season$mmh24[row] > 0) count = count+1
    row = row + 1
  }
  L_obs[i]=count
  count = 0
}

row=1
count=0
D_obs = rep(NA,2025-1960)
for (i in 1:65){
  for (j in 1:N_obs[i]){
    if (season$mmh24[row] > 0) count = count+season$mmh24[row]
    row = row + 1
  }
  D_obs[i]=count
  count = 0
}

Xraw = as.matrix(data.frame(sst$sst,soi$soi,nao$nao,nino$nino))
```

```{r}
# install.packages(c("rstan","bayesplot","loo"))
library(rstan); library(bayesplot); library(loo)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

# ---- Preprocess covariates: scale (important if priors are vague) ----
scale_cov <- function(x) if(is.numeric(x)) as.numeric((x - mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)) else x
Xscaled <- as.matrix(apply(Xraw, 2, scale_cov))
# ensure intercept: add column of 1s as first column if you want intercept
if (!all(Xscaled[,1] == 1)) Xscaled <- cbind(intercept = 1, Xscaled)

stan_data <- list(
  N_years = length(N_obs),
  N_obs = as.integer(N_obs),
  L_obs = as.integer(L_obs),
  D_obs = as.numeric(D_obs),
  P = ncol(Xscaled),
  X = Xscaled
)


# ---- Compile and sample ----
stan_file <- "~/Downloads/hurricane_project/season1_model.stan"  # path to saved stan code
fit <- stan(file = stan_file,
            data = stan_data,
            iter = 4000, warmup = 2000,
            chains = 4, thin = 1,
            control = list(adapt_delta = 0.95, max_treedepth = 15))

print(fit, pars = c("beta", "r", "theta", "mu", "sigma"), probs = c(0.025, 0.5, 0.975))

# ---- Diagnostics ----
# traceplots
traceplot(fit, pars = c("r", "theta", "mu", "sigma"))

# extract posterior
post <- extract(fit)
# posterior predictive checks (simple)
# simulate replicated D for each posterior draw is left for you; a simple check:
log_lik_matrix <- extract(fit, "log_lik")$log_lik  # N_draws x N_years
loo_res <- loo(log_lik_matrix)
print(loo_res)
```
```{r}
yr = 58
post = rstan::extract(fit)
S = length(post$theta)

# Compute lambda for each posterior draw
lambda_post = exp(Xscaled[yr,] %*% t(post$beta))
lambda_post = as.numeric(lambda_post)

# Predictive draws
N_pred = rnbinom(S, mu=lambda_post, size=post$r)
L_pred = rbinom(S, size = N_pred, prob=post$theta)

D_pred = numeric(S)
for (s in 1:S) {
  if (N_pred[s] == 0) D_pred[s] <- 0
  else {
    p_pos <- 1 - (1 - post$theta[s])^N_pred[s]
    if (runif(1) < p_pos) {
      D_pred[s] <- rlnorm(1, meanlog=post$mu[s], sdlog=post$sigma[s])
    } else {
      D_pred[s] <- 0
    }
  }
}

# observed values
N_obs_i <- N_obs[i]
L_obs_i <- L_obs[i]
D_obs_i <- D_obs[i]

```

```{r}
library(ggplot2)

dfN <- data.frame(N=N_pred)

ggplot(dfN, aes(x=N)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = N_obs_i, color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="N", y="Density"
  ) +
  theme_bw()


dfL <- data.frame(L=L_pred)

ggplot(dfL, aes(x=L)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = L_obs_i, color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Landfall/Damaging Cyclone Frequency",
    x="L", y="Density"
  ) +
  theme_bw()


# Probability of zero damage
p_zero <- mean(D_pred == 0)
p_pos  <- 1 - p_zero

dfD <- data.frame(D = D_pred)
dfD = mutate(dfD,logD = ifelse(D_pred > 0,log(D_pred),0))

ggplot(dfD, aes(x=logD)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, fill="lightblue", color="black") +
  geom_vline(xintercept = log(D_obs_i), color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Damage (log scale)",
    x="log(Damage)", y="Density"
  ) +
  annotate("text", x=Inf, y=Inf, hjust=1.1, vjust=2,
           label=sprintf("%.2f chance of $0 Damage", p_zero),
           size=4) +
  annotate("text", x=Inf, y=Inf, hjust=1.1, vjust=4,
           label=sprintf("%.2f chance of log(Damage)\nin this distribution", p_pos),
           size=4) +
  theme_bw()

```
```{r}
F_hat0 = function(param_post){mean(param_post<=0)}
depth_1D = function(param_post) 4 * F_hat0(param_post) * (1-F_hat0(param_post)) 


for (i in 1:5){
  print(names(post[i]))
  for (j in 1:ncol(as.matrix(post[[i]]))) print(depth_1D(as.matrix(post[[i]])[,j]))
}
```


```{r}
# GEV
df = merge(loss,met,by=c("storm_year","storm_name","storm_basin"))
df = df[df$storm_year >= 1960,]
df = df[,-3]

# removing multiple landfalls, not relevant for this model
df = filter(df, lf_id == 1)
d = duplicated(df$hurdatId)
df = df[!d,]

X_year = cbind(Xraw, storm_year = 1960:2024)

df = merge(df, X_year, by="storm_year")
df = filter(df, is.na(windspeed)==FALSE, is.na(pressure)==FALSE)
df = mutate(df, month = month(datetime))
X_raw2 = df[,c(28:34,1)]

df = mutate(df, log_minCP = log(pressure), log_maxWS = log(windspeed), log_damage = log(mmh24))
```
```{r}

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

# --- Load / prepare your data (example) ---
# df should have columns: log_minCP, log_maxWS, log_damage, plus covariates
# Covariate list (p = 11): intercept + avgLat, avgLon, startMonth, year, NAO, SOI, AMO, ANOM3.4, Atl_SST, Sunspots
# Make sure covariates are scaled/centered as desired
# Example:
# df <- read.csv("cyclone_individual_data.csv")
# create intercept
X_raw2$intercept <- 1
cov_names <- c( "lat", "lon", "sst", "soi", "nao", "nino", "month","year","intercept")
colnames(X_raw2)=cov_names
p <- length(cov_names)

# Scale covariates (except intercept)
scale_cols <- setdiff(cov_names, "intercept")
X_raw2[scale_cols] <- scale(X_raw2[scale_cols])

X <- as.matrix(X_raw2[, cov_names])
X <- cbind(X[,9],X[,1:8])
cov_names <- c( "intercept","lat", "lon", "sst", "soi", "nao", "nino", "month","year")

Z1 <- df$log_minCP
Y1 <- df$log_maxWS
Y2 <- df$log_damage

stan_data <- list(
  N = nrow(df),
  p = p,
  X = X,
  Z1 = as.numeric(Z1),
  Y1 = as.numeric(Y1),
  Y2 = as.numeric(Y2)
)

# --- Compile and fit ---
init_fun <- function() {
  list(
    alpha = rep(0, p),
    beta  = rep(0, p+1),
    gamma = rep(0, p+2),

    inv_sigma2_z1 = 1,   # sigma = 1
    inv_sigma2_y1 = 1,
    inv_sigma2_y2 = 1,

    xi_z1 = 0,
    xi_y1 = 0,
    xi_y2 = 0
  )
}

stan_file <- "hurricane_project/gev_individual.stan"
fit <- stan(file = stan_file,
            data = stan_data,
            chains = 4, iter = 4000, warmup = 2000,
            init = init_fun,
            control = list(adapt_delta = 0.95, max_treedepth = 15))

print(fit, pars = c("alpha", "beta", "gamma", "sigma_z1_out", "sigma_y1_out", "sigma_y2_out",
                    "xi_z1", "xi_y1", "xi_y2"), probs = c(0.025, 0.5, 0.975))

```
```{r}
library(extRemes)
post <- rstan::extract(fit)
mu_z1 = post$alpha %*% X[94,]
mu_y1 = post$beta %*% c(Z1[94],X[94,])
mu_y2 = post$gamma %*% c(Z1[94],Y1[94],X[94,])

mean(mu_z1)
mean(mu_y1)
mean(mu_y2)

Z1_harvey = revd(S,loc=mu_z1,scale=post$sigma_z1,shape=post$xi_z1,type="GEV")
Y1_harvey = revd(S,loc=mu_y1,scale=post$sigma_y1,shape=post$xi_y1,type="GEV")
Y2_harvey = revd(S,loc=mu_y2,scale=post$sigma_y2,shape=post$xi_y2,type="GEV")


dfp <- data.frame(log_pressure=Z1_harvey)

ggplot(dfp, aes(x=log_pressure)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = Z1[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_pressure", y="Density"
  ) +
  theme_bw()

dfw <- data.frame(log_ws=Y1_harvey)

ggplot(dfw, aes(x=log_ws)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=20, color="black", fill="lightblue") +
  geom_vline(xintercept = Y1[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_ws", y="Density"
  ) +
  theme_bw()

dfd <- data.frame(log_damage=Y2_harvey)

ggplot(dfd, aes(x=log_damage)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=20, color="black", fill="lightblue") +
  geom_vline(xintercept = Y2[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_damage", y="Density"
  ) +
  theme_bw()
```

```{r}
# Required packages
library(rstan)       # for extract
library(dplyr)
library(tidyr)
library(knitr)

# ---------- Utility: 1D depth ----------
one_d_depth <- function(draws) {
  F0 <- mean(draws <= 0)
  4 * F0 * (1 - F0)
}

# ---------- Main function ----------
# fit: stanfit object
# cov_names: character vector length p with covariate names (including "Intercept")
# returns: data.frame matching paper Table 1 (rows in same conceptual order)
build_table1_individual <- function(fit, cov_names) {
  post <- rstan::extract(fit, permuted = TRUE)
  # Ensure expected named elements exist
  stopifnot(!is.null(post$alpha), !is.null(post$beta), !is.null(post$gamma))
  # p
  p <- length(cov_names)
  # Check sizes
  if (ncol(post$alpha) != p) stop("post$alpha should have p columns matching cov_names")
  if (ncol(post$beta) != (p + 1)) stop("post$beta should have p+1 columns (beta[1]=Z1, then p covariates)")
  if (ncol(post$gamma) != (p + 2)) stop("post$gamma should have p+2 columns (gamma[1]=Z1,gamma[2]=Y1, then p covariates)")

  # compute depths
  depth_alpha <- apply(post$alpha, 2, one_d_depth)           # length p
  depth_beta_z1 <- one_d_depth(post$beta[,1])                # scalar
  depth_beta_X  <- apply(post$beta[, 2:(p+1)], 2, one_d_depth) # length p
  depth_gamma_z1 <- one_d_depth(post$gamma[,1])
  depth_gamma_y1 <- one_d_depth(post$gamma[,2])
  depth_gamma_X  <- apply(post$gamma[, 3:(p+2)], 2, one_d_depth)

  depth_xi <- c(
    one_d_depth(post$xi_z1),
    one_d_depth(post$xi_y1),
    one_d_depth(post$xi_y2)
  )
  depth_sigma <- c(
    one_d_depth(post$sigma_z1_out),
    one_d_depth(post$sigma_y1_out),
    one_d_depth(post$sigma_y2_out)
  )

  # Build rows in the same conceptual order as paper:
  # 1. Intercept (and other covariates) for Min CP (alpha block): p rows
  # 2. "Min CP (scaled)" (this is beta[1]) then covariates for MaxWS (beta X): 1 + p
  # 3. "Min CP (scaled)", "Max WS (scaled)" then covariates for Damages (gamma X): 2 + p
  # 4. xi and sigma rows

  # Row labels
  rows_alpha <- cov_names                       # p
  rows_beta  <- c("Min CP (scaled)", cov_names) # 1 + p
  rows_gamma <- c("Min CP (scaled)", "Max WS (scaled)", cov_names) # 2 + p
  rows_xi_sigma <- c("ξ", "σ")  # we will expand to 3 xi & 3 sigma rows across columns

  # Build a full row order vector (paper lists covariates once, but shows columns across three models)
  # We'll produce a table with one combined set of variable rows:
  # We'll use the covariate list once and then add two special "Min CP (scaled)" and "Max WS (scaled)" rows
  # The paper shows covariates in a single block; to match layout we will output a table where each row
  # corresponds to a covariate or special row, and columns correspond to MinCP/MaxWS/Damages.

  # Primary variable rows: covariates once, plus 2 special rows where they appear NA as needed.
  primary_rows <- c("Intercept", cov_names[cov_names != "Intercept"]) # ensure intercept first
  # But the paper lists Min CP (scaled) and Max WS (scaled) as rows too, so we append them below covariates to match their layout.
  # We'll mimic the PDF layout: first Intercept, then covariates (Min CP and Max WS appear as NA where not applicable),
  main_rows <- primary_rows

  # Build empty dataframe
  tableN <- data.frame(
    Variable = character(),
    MinCP = numeric(),
    MaxWS = numeric(),
    Damages = numeric(),
    stringsAsFactors = FALSE
  )

  # 1) add covariates rows (these correspond to alpha coefficients for MinCP, and also beta_X / gamma_X)
  for (i in seq_len(p)) {
    nm <- cov_names[i]
    mincp_val <- depth_alpha[i]
    maxws_val <- depth_beta_X[i]    # note: beta_X corresponds to covariates for MaxWS
    damage_val <- depth_gamma_X[i]
    tableN <- rbind(tableN, data.frame(Variable = nm,
                                       MinCP = mincp_val,
                                       MaxWS = maxws_val,
                                       Damages = damage_val,
                                       stringsAsFactors = FALSE))
  }

  # 2) insert rows for "Min CP (scaled)" and "Max WS (scaled)" that appear in paper
  # Paper shows "Min CP (scaled)" row: NA for MinCP column, MaxWS gets depth_beta_z1, Damages gets depth_gamma_z1
  # Paper shows "Max WS (scaled)" row: NA for MinCP and MaxWS columns; Damages gets depth for gamma_y1 or maybe gamma entry for MaxWS scaled
  # From excerpt, "Min CP (scaled)" listed under Variables with entries NA, 0.0000, 0.0177
  # and "Max WS (scaled)" listed with NA, NA, 0.2479
  # So we follow that: add two rows like the PDF.
  tableN <- rbind(
    tableN,
    data.frame(Variable = "Min CP (scaled)",
               MinCP = NA_real_,
               MaxWS = depth_beta_z1,
               Damages = depth_gamma_z1,
               stringsAsFactors = FALSE)
  )

  tableN <- rbind(
    tableN,
    data.frame(Variable = "Max WS (scaled)",
               MinCP = NA_real_,
               MaxWS = NA_real_,
               Damages = depth_gamma_y1,
               stringsAsFactors = FALSE)
  )

  # 3) xi and sigma rows:
  # The paper shows a single ξ and σ row but with three columns; we will add rows for ξ and σ and put values across columns.
  tableN <- rbind(
    tableN,
    data.frame(Variable = "ξ",
               MinCP = depth_xi[1],
               MaxWS = depth_xi[2],
               Damages = depth_xi[3],
               stringsAsFactors = FALSE)
  )

  tableN <- rbind(
    tableN,
    data.frame(Variable = "σ",
               MinCP = depth_sigma[1],
               MaxWS = depth_sigma[2],
               Damages = depth_sigma[3],
               stringsAsFactors = FALSE)
  )

  # Round for display to 4 decimals (as in paper)
  tableN$MinCP <- round(tableN$MinCP, 4)
  tableN$MaxWS <- round(tableN$MaxWS, 4)
  tableN$Damages <- round(tableN$Damages, 4)

  # Replace NaN with NA
  tableN[] <- lapply(tableN, function(x) replace(x, is.nan(x), NA))

  # Return
  return(tableN)
}

# ---------- Example usage ----------
# covariate names in same order as X in Stan (including intercept)
# e.g. cov_names <- c("Intercept","Avg.Lat","Avg.Lon","StartMonth","Year","NAO","SOI","AMO","ANOM3.4","Atl_SST","Sunspots")
# Provide the real covariate names you used
# cov_names <- c("Intercept", ...)

table1 <- build_table1_individual(fit, cov_names)
print(table1)


```


