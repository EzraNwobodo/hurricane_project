---
# title: "Untitled"
output: html_document
date: "2025-11-03"
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = '~/Downloads')
```

```{r}
library(tibble)
library(terra)
library(dplyr)
library(comprehenr)
library(lubridate)

setwd('~/Downloads')

end_year = 2024
amo = read.table("hurricane_project/amon.us.long.data.txt",strip.white = TRUE)
months = as.vector(amo[1,])
colnames(amo) = months
amo = amo[-1,]
amo = amo[amo$Year>=1960,]
amo = amo[amo$Year<=end_year,]
amo = amo %>% mutate(amo=(as.numeric(May)+as.numeric(Jun))/2 )

soi = read.table("hurricane_project/soi.txt",strip.white = TRUE)
colnames(soi) = months
soi = soi[soi$Year>=1960,]
soi = soi[soi$Year<=end_year,]
soi = soi %>% mutate(soi=(as.numeric(May)+as.numeric(Jun))/2 )

nao = read.table("hurricane_project/nao.data.txt",strip.white = TRUE)
colnames(nao) = months
nao[nao$Year>=1960,]
nao = nao[nao$Year>=1960,] 
nao = nao[nao$Year<=end_year,]
nao = nao %>% mutate(nao=(as.numeric(May)+as.numeric(Jun))/2 )

nino = read.table("hurricane_project/nino.ascii",strip.white = TRUE)
nino = nino[-1,c(1,2,9)]
foo = matrix(NA, nrow=76, ncol=12)
count=1
for (i in 1:76){
  for (j in 1:12){
    foo[i,j]=nino[count,3]
    count = count+1
    if(count==911) break
  }
}
foo = data.frame(foo)
nino = add_column(foo, Year = 1950:2025,.before="X1")
colnames(nino)=months
nino = nino[nino$Year>=1960,]
nino = nino[nino$Year<=end_year,]
nino = nino %>% mutate(nino=(as.numeric(May)+as.numeric(Jun))/2 )
```
```{}
tmpdir <- tempdir()
untar("hurricane_project/sst.tar.gz", exdir = tmpdir)

nc_files <- list.files(tmpdir, pattern = "\\.nc$", full.names = TRUE, recursive=TRUE)
nc_files

```
```{}

file <- nc_files[grepl("/tmp/RtmpXueFnm/data/netcdf3.0.2new/degree1/enh/sst.mean.nc", nc_files)]

r <- rast(file)
r

```
```{}
df1 <- as.data.frame(r, xy = TRUE)
head(df1)

```
```{}
write.csv(df1,"hurricane_project/sst_raw.csv", row.names = FALSE)
```
```{r}
setwd('~/Downloads')
list.files()
df1 = read.csv("hurricane_project/sst_raw.csv")
df1 = filter(df1, x >= 280  & x <= 340)
df1 = filter(df1, y >= 10  & y <= 25)
```
```{r}
month = seq(as.Date("1960-01-01"), as.Date("2025-07-01"), by="month")
sst_mean = to_vec(for(i in 3:789) mean(df1[,i],na.rm=T))

sst_month = data.frame(date=month, sst_mean=sst_mean)
sst_month = mutate(sst_month, year=year(date))

may_jun = to_vec(for(i in 1:66) (sst_month[12*(i-1)+5,2] + sst_month[12*(i-1)+6,2])/2 )
sst = data.frame(Year = 1960:2025, sst=may_jun)
sst = sst[sst$Year<=end_year,]
```


```{r}
setwd('~/Downloads')
loss = read.csv("hurricane_project/08302025_2024_aggregate.csv")
met = read.csv("hurricane_project/lf_matched_metrics-1.csv")
hurdat = read.csv("hurricane_project/hurdat_track-1761700598833-1.csv")

df = merge(loss,met,by=c("storm_year","storm_name","storm_basin"))
df = df[df$storm_year >= 1960,]
df = df[,-3]

hurdat = hurdat[hurdat$storm_year >= 1960,]

# removing multiple landfalls, not relevant for this model
df = filter(df, lf_id == 1)
d = duplicated(df$hurdatId)
df = df[!d,]
```

```{r}
hurdat2 = filter(hurdat, !(storm_status %in% c('TD','DB','LO','WV','SD')))
```

```{r}
name=''
for (i in 1:nrow(hurdat2)){
  if (hurdat2[i,]$storm_name != name) name = hurdat2[i,]$storm_name
  else hurdat2[i,]$storm_name=NA
}
```

```{r}
dupe = is.na(hurdat2$storm_name)
hurdat2 = hurdat2[!dupe,]
```
```{r}
hurdat2 = mutate(hurdat2, ID=paste(as.character(storm_year),storm_name))
df = mutate(df, ID=paste(as.character(storm_year),storm_name))
```
```{r}
season = merge(hurdat2,df,by='ID',all=T)
season = season[,c(3,4,5,30,31)]
colnames(season) = c('storm_year','storm_name','datetime','mmh24','mmp24')
season = season %>% mutate(storm_year_alt = ifelse(month(datetime)==1,storm_year-1,storm_year),mmp24 = ifelse(is.na(mmp24),0,mmp24),mmh24 = ifelse(is.na(mmh24),0,mmh24))

```
```{r}
# dropped amo because data only until 2022, fix later
# data prep

N_obs = rep(NA,2025-1960)
count=0
year = 1960
for (i in season$storm_year_alt){
  if(i == year) count = count+1
  else{
    year = i
    N_obs[year-1960] = count
    count = 1
  }
}
N_obs[65] = count

row=1
count=0
L_obs = rep(NA,2025-1960)
for (i in 1:65){
  for (j in 1:N_obs[i]){
    if (season$mmh24[row] > 0) count = count+1
    row = row + 1
  }
  L_obs[i]=count
  count = 0
}

row=1
count=0
D_obs = rep(NA,2025-1960)
for (i in 1:65){
  for (j in 1:N_obs[i]){
    if (season$mmh24[row] > 0) count = count+season$mmh24[row]
    row = row + 1
  }
  D_obs[i]=count
  count = 0
}

Xraw = as.matrix(data.frame(sst$sst,soi$soi,nao$nao,nino$nino))
```

```{r}
# install.packages(c("rstan","bayesplot","loo"))
library(rstan); library(bayesplot); library(loo)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

# ---- Preprocess covariates: scale (important if priors are vague) ----
scale_cov <- function(x) if(is.numeric(x)) as.numeric((x - mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)) else x
Xscaled <- as.matrix(apply(Xraw, 2, scale_cov))
# ensure intercept: add column of 1s as first column if you want intercept
if (!all(Xscaled[,1] == 1)) Xscaled <- cbind(intercept = 1, Xscaled)

stan_data <- list(
  N_years = length(N_obs),
  N_obs = as.integer(N_obs),
  L_obs = as.integer(L_obs),
  D_obs = as.numeric(D_obs),
  P = ncol(Xscaled),
  X = Xscaled
)


# ---- Compile and sample ----
stan_file <- "~/Downloads/hurricane_project/season1_model.stan"  # path to saved stan code
fit <- stan(file = stan_file,
            data = stan_data,
            iter = 4000, warmup = 2000,
            chains = 4, thin = 1,
            control = list(adapt_delta = 0.95, max_treedepth = 15))

print(fit, pars = c("beta", "r", "theta", "mu", "sigma"), probs = c(0.025, 0.5, 0.975))

# ---- Diagnostics ----
# traceplots
traceplot(fit, pars = c("r", "theta", "mu", "sigma"))

# extract posterior
post <- extract(fit)
# posterior predictive checks (simple)
# simulate replicated D for each posterior draw is left for you; a simple check:
log_lik_matrix <- extract(fit, "log_lik")$log_lik  # N_draws x N_years
loo_res <- loo(log_lik_matrix)
print(loo_res)
```
```{r}
yr = 58
post = rstan::extract(fit)
S = length(post$theta)

# Compute lambda for each posterior draw
lambda_post = exp(Xscaled[yr,] %*% t(post$beta))
lambda_post = as.numeric(lambda_post)

# Predictive draws
N_pred = rnbinom(S, mu=lambda_post, size=post$r)
L_pred = rbinom(S, size = N_pred, prob=post$theta)

D_pred = numeric(S)
for (s in 1:S) {
  if (N_pred[s] == 0) D_pred[s] <- 0
  else {
    p_pos <- 1 - (1 - post$theta[s])^N_pred[s]
    if (runif(1) < p_pos) {
      D_pred[s] <- rlnorm(1, meanlog=post$mu[s], sdlog=post$sigma[s])
    } else {
      D_pred[s] <- 0
    }
  }
}

# observed values
N_obs_i <- N_obs[i]
L_obs_i <- L_obs[i]
D_obs_i <- D_obs[i]

```

```{r}
library(ggplot2)

dfN <- data.frame(N=N_pred)

ggplot(dfN, aes(x=N)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = N_obs_i, color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="N", y="Density"
  ) +
  theme_bw()


dfL <- data.frame(L=L_pred)

ggplot(dfL, aes(x=L)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = L_obs_i, color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Landfall/Damaging Cyclone Frequency",
    x="L", y="Density"
  ) +
  theme_bw()


# Probability of zero damage
p_zero <- mean(D_pred == 0)
p_pos  <- 1 - p_zero

dfD <- data.frame(D = D_pred)
dfD = mutate(dfD,logD = ifelse(D_pred > 0,log(D_pred),0))

ggplot(dfD, aes(x=logD)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, fill="lightblue", color="black") +
  geom_vline(xintercept = log(D_obs_i), color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Damage (log scale)",
    x="log(Damage)", y="Density"
  ) +
  annotate("text", x=Inf, y=Inf, hjust=1.1, vjust=2,
           label=sprintf("%.2f chance of $0 Damage", p_zero),
           size=4) +
  annotate("text", x=Inf, y=Inf, hjust=1.1, vjust=4,
           label=sprintf("%.2f chance of log(Damage)\nin this distribution", p_pos),
           size=4) +
  theme_bw()

```
```{r}
F_hat0 = function(param_post){mean(param_post<=0)}
depth_1D = function(param_post) 4 * F_hat0(param_post) * (1-F_hat0(param_post)) 


for (i in 1:5){
  print(names(post[i]))
  for (j in 1:ncol(as.matrix(post[[i]]))) print(depth_1D(as.matrix(post[[i]])[,j]))
}
```


```{r}
# GEV
df = merge(loss,met,by=c("storm_year","storm_name","storm_basin"))
df = df[df$storm_year >= 1960,]
df = df[,-3]

# removing multiple landfalls, not relevant for this model
df = filter(df, lf_id == 1)
d = duplicated(df$hurdatId)
df = df[!d,]

X_year = cbind(Xraw, storm_year = 1960:2024)

df = merge(df, X_year, by="storm_year")
df = filter(df, is.na(windspeed)==FALSE, is.na(pressure)==FALSE)
df = mutate(df, month = month(datetime))
X_raw2 = df[,c(28:34,1)]

df = mutate(df, log_minCP = log(pressure), log_maxWS = log(windspeed), log_damage = log(mmh24))
```
```{r}
library(rstan); library(bayesplot); library(loo)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

# --- Load / prepare your data (example) ---
# df should have columns: log_minCP, log_maxWS, log_damage, plus covariates
# Covariate list (p = 11): intercept + avgLat, avgLon, startMonth, year, NAO, SOI, AMO, ANOM3.4, Atl_SST, Sunspots
# Make sure covariates are scaled/centered as desired
# Example:
# df <- read.csv("cyclone_individual_data.csv")
# create intercept
X_raw2$intercept <- 1
cov_names <- c( "lat", "lon", "sst", "soi", "nao", "nino", "month","year","intercept")
colnames(X_raw2)=cov_names
p <- length(cov_names)

# Scale covariates (except intercept)
scale_cols <- setdiff(cov_names, "intercept")
X_raw2[scale_cols] <- scale(X_raw2[scale_cols])

X <- as.matrix(X_raw2[, cov_names])
X <- cbind(X[,9],X[,1:8])
cov_names <- c( "intercept","lat", "lon", "sst", "soi", "nao", "nino", "month","year")

Z1 <- df$log_minCP 
Z1_mean <- mean(Z1)
Z1 <- Z1 - mean(Z1)

Y1 <- df$log_maxWS
Y1_mean <- mean(Y1)
Y1 <- Y1 - mean(Y1)

Y2 <- df$log_damage
Y2_mean <- mean(Y2)
Y2 <- Y2 - mean(Y2)

stan_data <- list(
  N = nrow(df),
  p = p,
  X = X,
  Z1 = as.numeric(Z1),
  Y1 = as.numeric(Y1),
  Y2 = as.numeric(Y2)
)

# --- Compile and fit ---
init_fun <- function() {
  list(
    alpha = rep(0, p),
    beta  = rep(0, p+1),
    gamma = rep(0, p+2),

    inv_sigma2_z1 = 1,   # sigma = 1
    inv_sigma2_y1 = 1,
    inv_sigma2_y2 = 1,

    xi_z1 = 0,
    xi_y1 = 0,
    xi_y2 = 0
  )
}

stan_file <- "hurricane_project/gev_individual.stan"
fit <- stan(file = stan_file,
            data = stan_data,
            init = init_fun,
            chains = 4, iter = 6000, warmup = 3000,
            control = list(adapt_delta = 0.999, max_treedepth = 15))


print(fit, pars = c("alpha", "beta", "gamma", "sigma_z1_out", "sigma_y1_out", "sigma_y2_out",
                    "xi_z1", "xi_y1", "xi_y2"), probs = c(0.025, 0.5, 0.975))

saveRDS(fit, "hurricane_project/fit_no_ike.rds")
```
```{r}
library(posterior)

library(bayesplot)

color_scheme_set("red")
mcmc_pairs(
  as_draws(fit),
  pars = c("xi_z1", "xi_y1", "xi_y2",
           "log_sigma_z1", "log_sigma_y1", "log_sigma_y2",
           "alpha[1]", "beta[1]", "gamma[1]"),
  np = nuts_params(fit)
)


```
```{r}
library(bayesplot)

mcmc_trace(
  as.array(fit),
  pars = c("xi_z1", "xi_y1", "xi_y2",
           "log_sigma_z1", "log_sigma_y1", "log_sigma_y2",
           "alpha[1]", "beta[1]", "gamma[1]","gamma[2]","gamma[3]")
)

```

```{r}
library(extRemes)
post <- rstan::extract(fit)
mu_z1 = post$alpha %*% X[94,]
mu_y1 = post$beta %*% c(Z1[94],X[94,])
mu_y2 = post$gamma %*% c(Z1[94],Y1[94],X[94,])

mean(mu_z1)
mean(mu_y1)
mean(mu_y2)

S=12000
Z1_harvey = revd(S,loc=mu_z1,scale=post$sigma_z1,shape=post$xi_z1,type="GEV")
Y1_harvey = revd(S,loc=mu_y1,scale=post$sigma_y1,shape=post$xi_y1,type="GEV")
Y2_harvey = revd(S,loc=mu_y2,scale=post$sigma_y2,shape=post$xi_y2,type="GEV")


dfp <- data.frame(log_pressure=Z1_harvey)

ggplot(dfp, aes(x=log_pressure)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=40, color="black", fill="lightblue") +
  geom_vline(xintercept = Z1[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_pressure", y="Density"
  ) +
  theme_bw()

dfw <- data.frame(log_ws=Y1_harvey)

ggplot(dfw, aes(x=log_ws)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=20, color="black", fill="lightblue") +
  geom_vline(xintercept = Y1[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_ws", y="Density"
  ) +
  theme_bw()

dfd <- data.frame(log_damage=Y2_harvey)

ggplot(dfd, aes(x=log_damage)) +
  geom_histogram(aes(y=after_stat(density)),
                 bins=20, color="black", fill="lightblue") +
  geom_vline(xintercept = Y2[94], color="red", linetype="dashed", linewidth=1) +
  labs(
    title="Posterior Predictive: Storm Frequency",
    x="log_damage", y="Density"
  ) +
  theme_bw()
```

```{r}
# Required packages
library(rstan)       # for extract
library(dplyr)
library(tidyr)
library(knitr)

# ---------- Utility: 1D depth ----------
one_d_depth <- function(draws) {
  F0 <- mean(draws <= 0)
  4 * F0 * (1 - F0)
}

# ---------- Main function ----------
# fit: stanfit object
# cov_names: character vector length p with covariate names (including "Intercept")
# returns: data.frame matching paper Table 1 (rows in same conceptual order)
build_table1_individual <- function(fit, cov_names) {
  post <- rstan::extract(fit, permuted = TRUE)
  # Ensure expected named elements exist
  stopifnot(!is.null(post$alpha), !is.null(post$beta), !is.null(post$gamma))
  # p
  p <- length(cov_names)
  # Check sizes
  if (ncol(post$alpha) != p) stop("post$alpha should have p columns matching cov_names")
  if (ncol(post$beta) != (p + 1)) stop("post$beta should have p+1 columns (beta[1]=Z1, then p covariates)")
  if (ncol(post$gamma) != (p + 2)) stop("post$gamma should have p+2 columns (gamma[1]=Z1,gamma[2]=Y1, then p covariates)")

  # compute depths
  depth_alpha <- apply(post$alpha, 2, one_d_depth)           # length p
  depth_beta_z1 <- one_d_depth(post$beta[,1])                # scalar
  depth_beta_X  <- apply(post$beta[, 2:(p+1)], 2, one_d_depth) # length p
  depth_gamma_z1 <- one_d_depth(post$gamma[,1])
  depth_gamma_y1 <- one_d_depth(post$gamma[,2])
  depth_gamma_X  <- apply(post$gamma[, 3:(p+2)], 2, one_d_depth)

  depth_xi <- c(
    one_d_depth(post$xi_z1),
    one_d_depth(post$xi_y1),
    one_d_depth(post$xi_y2)
  )
  depth_sigma <- c(
    one_d_depth(post$sigma_z1_out),
    one_d_depth(post$sigma_y1_out),
    one_d_depth(post$sigma_y2_out)
  )

  # Build rows in the same conceptual order as paper:
  # 1. Intercept (and other covariates) for Min CP (alpha block): p rows
  # 2. "Min CP (scaled)" (this is beta[1]) then covariates for MaxWS (beta X): 1 + p
  # 3. "Min CP (scaled)", "Max WS (scaled)" then covariates for Damages (gamma X): 2 + p
  # 4. xi and sigma rows

  # Row labels
  rows_alpha <- cov_names                       # p
  rows_beta  <- c("Min CP (scaled)", cov_names) # 1 + p
  rows_gamma <- c("Min CP (scaled)", "Max WS (scaled)", cov_names) # 2 + p
  rows_xi_sigma <- c("ξ", "σ")  # we will expand to 3 xi & 3 sigma rows across columns

  # Build a full row order vector (paper lists covariates once, but shows columns across three models)
  # We'll produce a table with one combined set of variable rows:
  # We'll use the covariate list once and then add two special "Min CP (scaled)" and "Max WS (scaled)" rows
  # The paper shows covariates in a single block; to match layout we will output a table where each row
  # corresponds to a covariate or special row, and columns correspond to MinCP/MaxWS/Damages.

  # Primary variable rows: covariates once, plus 2 special rows where they appear NA as needed.
  primary_rows <- c("Intercept", cov_names[cov_names != "Intercept"]) # ensure intercept first
  # But the paper lists Min CP (scaled) and Max WS (scaled) as rows too, so we append them below covariates to match their layout.
  # We'll mimic the PDF layout: first Intercept, then covariates (Min CP and Max WS appear as NA where not applicable),
  main_rows <- primary_rows

  # Build empty dataframe
  tableN <- data.frame(
    Variable = character(),
    MinCP = numeric(),
    MaxWS = numeric(),
    Damages = numeric(),
    stringsAsFactors = FALSE
  )

  # 1) add covariates rows (these correspond to alpha coefficients for MinCP, and also beta_X / gamma_X)
  for (i in seq_len(p)) {
    nm <- cov_names[i]
    mincp_val <- depth_alpha[i]
    maxws_val <- depth_beta_X[i]    # note: beta_X corresponds to covariates for MaxWS
    damage_val <- depth_gamma_X[i]
    tableN <- rbind(tableN, data.frame(Variable = nm,
                                       MinCP = mincp_val,
                                       MaxWS = maxws_val,
                                       Damages = damage_val,
                                       stringsAsFactors = FALSE))
  }

  # 2) insert rows for "Min CP (scaled)" and "Max WS (scaled)" that appear in paper
  # Paper shows "Min CP (scaled)" row: NA for MinCP column, MaxWS gets depth_beta_z1, Damages gets depth_gamma_z1
  # Paper shows "Max WS (scaled)" row: NA for MinCP and MaxWS columns; Damages gets depth for gamma_y1 or maybe gamma entry for MaxWS scaled
  # From excerpt, "Min CP (scaled)" listed under Variables with entries NA, 0.0000, 0.0177
  # and "Max WS (scaled)" listed with NA, NA, 0.2479
  # So we follow that: add two rows like the PDF.
  tableN <- rbind(
    tableN,
    data.frame(Variable = "Min CP (scaled)",
               MinCP = NA_real_,
               MaxWS = depth_beta_z1,
               Damages = depth_gamma_z1,
               stringsAsFactors = FALSE)
  )

  tableN <- rbind(
    tableN,
    data.frame(Variable = "Max WS (scaled)",
               MinCP = NA_real_,
               MaxWS = NA_real_,
               Damages = depth_gamma_y1,
               stringsAsFactors = FALSE)
  )

  # 3) xi and sigma rows:
  # The paper shows a single ξ and σ row but with three columns; we will add rows for ξ and σ and put values across columns.
  tableN <- rbind(
    tableN,
    data.frame(Variable = "ξ",
               MinCP = depth_xi[1],
               MaxWS = depth_xi[2],
               Damages = depth_xi[3],
               stringsAsFactors = FALSE)
  )

  tableN <- rbind(
    tableN,
    data.frame(Variable = "σ",
               MinCP = depth_sigma[1],
               MaxWS = depth_sigma[2],
               Damages = depth_sigma[3],
               stringsAsFactors = FALSE)
  )

  # Round for display to 4 decimals (as in paper)
  tableN$MinCP <- round(tableN$MinCP, 4)
  tableN$MaxWS <- round(tableN$MaxWS, 4)
  tableN$Damages <- round(tableN$Damages, 4)

  # Replace NaN with NA
  tableN[] <- lapply(tableN, function(x) replace(x, is.nan(x), NA))

  # Return
  return(tableN)
}

# ---------- Example usage ----------
# covariate names in same order as X in Stan (including intercept)
# e.g. cov_names <- c("Intercept","Avg.Lat","Avg.Lon","StartMonth","Year","NAO","SOI","AMO","ANOM3.4","Atl_SST","Sunspots")
# Provide the real covariate names you used
# cov_names <- c("Intercept", ...)

table1 <- build_table1_individual(fit, cov_names)
print(table1)


```

```{r}
# ---------------------------------------------
# Metropolis–Hastings sampler for individual GEV model
# Using your data format:
#   X   : N x p matrix with columns:
#         cov_names <- c("intercept","lat","lon","sst","soi","nao","nino","month","year")
#         (non-intercept columns already scaled)
#   Z1  : df$log_minCP
#   Y1  : df$log_maxWS
#   Y2  : df$log_damage
# ---------------------------------------------

library(MASS)

# ---- 1. Data assumptions ----
# X, Z1, Y1, Y2 must exist in your environment
cov_names <- c("intercept","lat","lon","sst","soi","nao","nino","month","year")

X  <- as.matrix(X)
Z1 <- as.numeric(Z1)
Y1 <- as.numeric(Y1)
Y2 <- as.numeric(Y2)

N <- nrow(X)
p <- ncol(X)
stopifnot(p == length(cov_names))

# ---- 2. GEV log-pdf (scalar) ----
gev_logpdf_vec <- function(x, mu, sigma, xi) {
  # x, mu, sigma, xi: vectors of same length or scalars recycled
  x   <- as.numeric(x)
  mu  <- as.numeric(mu)
  sigma <- as.numeric(sigma)
  xi  <- as.numeric(xi)
  n <- length(x)
  out <- numeric(n)
  for (i in seq_len(n)) {
    xi_i <- xi[min(i, length(xi))]
    mu_i <- mu[min(i, length(mu))]
    s_i  <- sigma[min(i, length(sigma))]
    x_i  <- x[i]
    if (s_i <= 0) {
      out[i] <- -Inf
    } else if (abs(xi_i) < 1e-10) {
      # Gumbel limit
      t <- (x_i - mu_i) / s_i
      out[i] <- -log(s_i) - t - exp(-t)
    } else {
      z <- 1 + xi_i * ((x_i - mu_i) / s_i)
      if (z <= 0) {
        out[i] <- -Inf
      } else {
        inv_xi <- 1 / xi_i
        logz   <- log(z)
        out[i] <- -log(s_i) + (-1 - inv_xi) * logz - exp(-inv_xi * logz)
      }
    }
  }
  out
}

# ---- 3. Transform xi from unconstrained u to bounded interval ----
xi_z1_lower <- -1
xi_z1_upper <-  1
xi_y_lower  <- -0.55
xi_y_upper  <-  0.5

inv_logit <- function(u) 1 / (1 + exp(-u))

u_to_xi_z1 <- function(u) {
  xi_z1_lower + (xi_z1_upper - xi_z1_lower) * inv_logit(u)
}
u_to_xi_y <- function(u) {
  xi_y_lower + (xi_y_upper - xi_y_lower) * inv_logit(u)
}

# ---- 4. Log-prior (up to additive constant) ----
log_prior <- function(par) {
  # par is a list with:
  # alpha[p], beta[p+1], gamma[p+2],
  # log_sigma_z1, log_sigma_y1, log_sigma_y2,
  # xi_z1_u, xi_y1_u, xi_y2_u

  lp <- 0

  # Priors from paper:
  # alpha ~ N(0, 10^2)
  # beta  ~ N(0, 10^3)
  # gamma ~ N(0, 10^2)
  lp <- lp + sum(dnorm(par$alpha, 0, 10,   log = TRUE))
  lp <- lp + sum(dnorm(par$beta,  0, 1000, log = TRUE))
  lp <- lp + sum(dnorm(par$gamma, 0, 10,   log = TRUE))

  # 1/sigma^2 ~ Gamma(1, 3)
  # t = 1/sigma^2 ; t ~ Gamma(shape=1, scale=3)
  # log p(sigma) = log p(t) + log |dt/dsigma|
  # dt/dsigma = -2/sigma^3 => |dt/dsigma| = 2/sigma^3
  for (ls in c("log_sigma_z1","log_sigma_y1","log_sigma_y2")) {
    sigma <- exp(par[[ls]])
    t <- 1 / (sigma^2)
    lp <- lp + dgamma(t, shape = 1, scale = 3, log = TRUE) + log(2) - 3 * log(sigma)
  }

  # Uniform priors on xi with bounds; in u-space we include Jacobian:
  # xi = a + (b-a)*inv_logit(u)
  # dxi/du = (b-a)*inv_logit(u)*(1-inv_logit(u))
  # log p(u) = const + log(dxi/du)
  u <- par$xi_z1_u
  lp <- lp + log((xi_z1_upper - xi_z1_lower) *
                 inv_logit(u) * (1 - inv_logit(u)))

  for (u_name in c("xi_y1_u","xi_y2_u")) {
    uu <- par[[u_name]]
    lp <- lp + log((xi_y_upper - xi_y_lower) *
                   inv_logit(uu) * (1 - inv_logit(uu)))
  }

  lp
}

# ---- 5. Log-likelihood ----
log_lik <- function(par) {
  # unpack
  alpha <- par$alpha
  beta  <- par$beta
  gamma <- par$gamma

  mu_z1 <- as.numeric(X %*% alpha)
  mu_y1 <- beta[1] * Z1 + as.numeric(X %*% beta[2:(p+1)])
  mu_y2 <- gamma[1] * Z1 + gamma[2] * Y1 + as.numeric(X %*% gamma[3:(p+2)])

  sigma_z1 <- exp(par$log_sigma_z1)
  sigma_y1 <- exp(par$log_sigma_y1)
  sigma_y2 <- exp(par$log_sigma_y2)

  xi_z1 <- u_to_xi_z1(par$xi_z1_u)
  xi_y1 <- u_to_xi_y(par$xi_y1_u)
  xi_y2 <- u_to_xi_y(par$xi_y2_u)

  ll_z1 <- gev_logpdf_vec(Z1, mu_z1, sigma_z1, rep(xi_z1, N))
  ll_y1 <- gev_logpdf_vec(Y1, mu_y1, sigma_y1, rep(xi_y1, N))
  ll_y2 <- gev_logpdf_vec(Y2, mu_y2, sigma_y2, rep(xi_y2, N))

  total <- sum(ll_z1) + sum(ll_y1) + sum(ll_y2)
  total
}

log_post <- function(par) {
  lp <- log_prior(par)
  if (!is.finite(lp)) return(-Inf)
  ll <- log_lik(par)
  if (!is.finite(ll)) return(-Inf)
  lp + ll
}

# ---- 6. Initial values ----
init_par <- function() {
  list(
    alpha = rep(0, p),
    beta  = rep(0, p + 1),
    gamma = rep(0, p + 2),
    log_sigma_z1 = log(sd(Z1)),
    log_sigma_y1 = log(sd(Y1)),
    log_sigma_y2 = log(sd(Y2)),
    xi_z1_u = 0,
    xi_y1_u = 0,
    xi_y2_u = 0
  )
}

# ---- 7. Random-walk proposals ----
propose_block <- function(curr, scales) {
  rnorm(length(curr), mean = curr, sd = scales)
}

# ---- 8. MH driver ----
run_mh <- function(
  Niter         = 100000,
  burnin        = 20000,
  thin          = 10,
  target_accept = 0.20,
  adapt_every   = 1000
) {
  Niter   <- as.integer(Niter)
  burnin  <- as.integer(burnin)
  thin    <- as.integer(thin)
  adapt_every <- as.integer(adapt_every)

  par_curr <- init_par()
  lp_curr  <- log_post(par_curr)
  if (!is.finite(lp_curr)) stop("Initial log posterior is -Inf; tweak init.")

  # Proposal scales (tuned during burn-in)
  scale_alpha <- rep(0.02, p)
  scale_beta  <- rep(0.02, p + 1)
  scale_gamma <- rep(0.02, p + 2)
  
  # smaller initial scales for intercepts & Z1/Y1 terms
  scale_alpha[1] <- 0.01         # alpha intercept
  scale_beta[1]  <- 0.01         # coef on Z1 in Y1 model
  scale_gamma[1] <- 0.01         # coef on Z1 in Y2 model
  scale_gamma[2] <- 0.01         # coef on Y1 in Y2 model

  scale_logs   <- rep(0.05, 3)
  scale_xi_u   <- rep(0.10, 3)

  # Acceptance counters per block
  acc_alpha <- acc_beta <- acc_gamma <- acc_logs <- acc_xi <- 0L
  tot_alpha <- tot_beta <- tot_gamma <- tot_logs <- tot_xi <- 0L

  # Storage indices
  keep_idx <- seq(burnin + 1, Niter, by = thin)
  n_keep   <- length(keep_idx)

  samples <- list(
    alpha     = matrix(NA_real_, nrow = n_keep, ncol = p),
    beta      = matrix(NA_real_, nrow = n_keep, ncol = p + 1),
    gamma     = matrix(NA_real_, nrow = n_keep, ncol = p + 2),
    log_sigma = matrix(NA_real_, nrow = n_keep, ncol = 3),
    xi_u      = matrix(NA_real_, nrow = n_keep, ncol = 3),
    logpost   = numeric(n_keep)
  )
  keep_i <- 0L

  for (iter in 1:Niter) {

    # ---- alpha block: component-wise updates ----
    for (j in 1:p) {
      tot_alpha <- tot_alpha + 1L
      prop_par <- par_curr
      # propose only alpha[j]
      prop_par$alpha[j] <- rnorm(1, par_curr$alpha[j], scale_alpha[j])
      lp_prop <- log_post(prop_par)
      if (is.finite(lp_prop) && log(runif(1)) < (lp_prop - lp_curr)) {
        par_curr <- prop_par
        lp_curr  <- lp_prop
        acc_alpha <- acc_alpha + 1L
      }
    }


    # ---- beta block: component-wise updates ----
    for (j in 1:(p+1)) {
      tot_beta <- tot_beta + 1L
      prop_par <- par_curr
      prop_par$beta[j] <- rnorm(1, par_curr$beta[j], scale_beta[j])
      lp_prop <- log_post(prop_par)
      if (is.finite(lp_prop) && log(runif(1)) < (lp_prop - lp_curr)) {
        par_curr <- prop_par
        lp_curr  <- lp_prop
        acc_beta <- acc_beta + 1L
      }
    }


    # ---- gamma block: component-wise updates ----
    for (j in 1:(p+2)) {
      tot_gamma <- tot_gamma + 1L
      prop_par <- par_curr
      prop_par$gamma[j] <- rnorm(1, par_curr$gamma[j], scale_gamma[j])
      lp_prop <- log_post(prop_par)
      if (is.finite(lp_prop) && log(runif(1)) < (lp_prop - lp_curr)) {
        par_curr <- prop_par
        lp_curr  <- lp_prop
        acc_gamma <- acc_gamma + 1L
      }
    }

    # ---- log sigma block (3) ----
    tot_logs <- tot_logs + 1L
    prop_par <- par_curr
    logs_vec <- c(par_curr$log_sigma_z1, par_curr$log_sigma_y1, par_curr$log_sigma_y2)
    logs_prop <- propose_block(logs_vec, scale_logs)
    prop_par$log_sigma_z1 <- logs_prop[1]
    prop_par$log_sigma_y1 <- logs_prop[2]
    prop_par$log_sigma_y2 <- logs_prop[3]
    lp_prop <- log_post(prop_par)
    if (is.finite(lp_prop) && log(runif(1)) < (lp_prop - lp_curr)) {
      par_curr <- prop_par
      lp_curr  <- lp_prop
      acc_logs <- acc_logs + 1L
    }

    # ---- xi_u block (3) ----
    tot_xi <- tot_xi + 1L
    prop_par <- par_curr
    xi_u_vec  <- c(par_curr$xi_z1_u, par_curr$xi_y1_u, par_curr$xi_y2_u)
    xi_u_prop <- propose_block(xi_u_vec, scale_xi_u)
    prop_par$xi_z1_u <- xi_u_prop[1]
    prop_par$xi_y1_u <- xi_u_prop[2]
    prop_par$xi_y2_u <- xi_u_prop[3]
    lp_prop <- log_post(prop_par)
    if (is.finite(lp_prop) && log(runif(1)) < (lp_prop - lp_curr)) {
      par_curr <- prop_par
      lp_curr  <- lp_prop
      acc_xi <- acc_xi + 1L
    }

    # ---- adaptive tuning during burn-in ----
    if (iter <= burnin && (iter %% adapt_every == 0)) {
      a_alpha <- acc_alpha / max(1, tot_alpha)
      a_beta  <- acc_beta  / max(1, tot_beta)
      a_gamma <- acc_gamma / max(1, tot_gamma)
      a_logs  <- acc_logs  / max(1, tot_logs)
      a_xi    <- acc_xi    / max(1, tot_xi)

      adapt <- function(scale, acc_rate) {
        if (is.na(acc_rate)) return(scale)
        if (acc_rate > target_accept + 0.05) {
          scale * 1.1
        } else if (acc_rate < target_accept - 0.05) {
          scale / 1.1
        } else {
          scale
        }
      }

      scale_alpha <- adapt(scale_alpha, a_alpha)
      scale_beta  <- adapt(scale_beta,  a_beta)
      scale_gamma <- adapt(scale_gamma, a_gamma)
      scale_logs  <- adapt(scale_logs,  a_logs)
      scale_xi_u  <- adapt(scale_xi_u,  a_xi)

      # reset counters for next adaptation window
      acc_alpha <- acc_beta <- acc_gamma <- acc_logs <- acc_xi <- 0L
      tot_alpha <- tot_beta <- tot_gamma <- tot_logs <- tot_xi <- 0L

      cat("adapt at iter", iter,
          "acc(alpha,beta,gamma,logs,xi)=",
          round(c(a_alpha,a_beta,a_gamma,a_logs,a_xi),3), "\n")
    }

    # ---- store post-burnin, thinned ----
    if (iter > burnin && ((iter - burnin) %% thin == 0)) {
      keep_i <- keep_i + 1L
      samples$alpha[keep_i, ]     <- par_curr$alpha
      samples$beta[keep_i, ]      <- par_curr$beta
      samples$gamma[keep_i, ]     <- par_curr$gamma
      samples$log_sigma[keep_i, ] <- c(par_curr$log_sigma_z1,
                                       par_curr$log_sigma_y1,
                                       par_curr$log_sigma_y2)
      samples$xi_u[keep_i, ]      <- c(par_curr$xi_z1_u,
                                       par_curr$xi_y1_u,
                                       par_curr$xi_y2_u)
      samples$logpost[keep_i]     <- lp_curr
    }

    if (iter %% (Niter / 10) == 0) {
      cat("iter", iter, "logpost", round(lp_curr, 1), "\n")
    }
  }

  # final acceptance rates over last adaptation window (rough)
  acc_rates <- c(
    alpha = acc_alpha / max(1, tot_alpha),
    beta  = acc_beta  / max(1, tot_beta),
    gamma = acc_gamma / max(1, tot_gamma),
    logs  = acc_logs  / max(1, tot_logs),
    xi    = acc_xi    / max(1, tot_xi)
  )

  list(samples = samples,
       final_par = par_curr,
       final_logpost = lp_curr,
       final_scales = list(
         alpha = scale_alpha,
         beta  = scale_beta,
         gamma = scale_gamma,
         logs  = scale_logs,
         xi_u  = scale_xi_u
       ),
       acc_rates = acc_rates)
}

# ---- 9. 1-D depth table (like Table 1) ----
compute_depth_table <- function(samples, cov_names) {
  s <- samples
  depth1d <- function(v) {
    F0 <- mean(v <= 0)
    4 * F0 * (1 - F0)
  }

  # transform xi_u -> xi
  xi_z1_vec <- u_to_xi_z1(s$xi_u[,1])
  xi_y1_vec <- u_to_xi_y(s$xi_u[,2])
  xi_y2_vec <- u_to_xi_y(s$xi_u[,3])

  # sigmas
  sigma_mat <- exp(s$log_sigma)

  depth_alpha     <- apply(s$alpha, 2, depth1d)
  depth_beta_z1   <- depth1d(s$beta[,1])
  depth_beta_X    <- apply(s$beta[,2:(p+1)], 2, depth1d)
  depth_gamma_z1  <- depth1d(s$gamma[,1])
  depth_gamma_y1  <- depth1d(s$gamma[,2])
  depth_gamma_X   <- apply(s$gamma[,3:(p+2)], 2, depth1d)
  depth_xi        <- c(depth1d(xi_z1_vec), depth1d(xi_y1_vec), depth1d(xi_y2_vec))
  depth_sigma     <- apply(sigma_mat, 2, depth1d)

  # main covariate rows
  tbl <- data.frame(
    Variable = cov_names,
    MinCP   = round(depth_alpha,   4),
    MaxWS   = round(depth_beta_X,  4),
    Damages = round(depth_gamma_X, 4),
    stringsAsFactors = FALSE
  )

  # rows for Min CP (scaled) and Max WS (scaled)
  tbl <- rbind(
    tbl,
    data.frame(Variable = "Min CP (scaled)",
               MinCP = NA,
               MaxWS = round(depth_beta_z1,  4),
               Damages = round(depth_gamma_z1,4),
               stringsAsFactors = FALSE),
    data.frame(Variable = "Max WS (scaled)",
               MinCP = NA,
               MaxWS = NA,
               Damages = round(depth_gamma_y1,4),
               stringsAsFactors = FALSE)
  )

  # xi and sigma rows
  tbl <- rbind(
    tbl,
    data.frame(Variable = "xi",
               MinCP = round(depth_xi[1],    4),
               MaxWS = round(depth_xi[2],    4),
               Damages = round(depth_xi[3],  4),
               stringsAsFactors = FALSE),
    data.frame(Variable = "sigma",
               MinCP = round(depth_sigma[1], 4),
               MaxWS = round(depth_sigma[2], 4),
               Damages = round(depth_sigma[3],4),
               stringsAsFactors = FALSE)
  )

  tbl
}

```
```{r}
# ---- 10. Example run ----
# First do a small test:
set.seed(123)
mh_test <- run_mh(
  Niter = 30000,   # try 50k first
  burnin = 10000,
  thin   = 10,
  target_accept = 0.20
)

mh_test$acc_rates  # block-wise acceptance rates (last window)

table1_test <- compute_depth_table(mh_test$samples, cov_names)
print(table1_test)

# For full paper-style run, increase Niter (e.g. 1e6) and maybe thin more.

```
```{r}
library(coda)

alpha_mat <- mh_test$samples$alpha  # n_keep x p

# pick a few components
par(mfrow=c(3,1))
plot(ts(alpha_mat[,1]), main="Trace: alpha[1]")
plot(ts(alpha_mat[,2]), main="Trace: alpha[2]")
plot(ts(alpha_mat[,3]), main="Trace: alpha[3]")

effectiveSize(alpha_mat[,1])
effectiveSize(alpha_mat[,2])

```
```{r}
beta_mat <- mh_test$samples$beta  # n_keep x p

# pick a few components
par(mfrow=c(3,1))
plot(ts(beta_mat[,1]), main="Trace: beta[1]")
plot(ts(beta_mat[,2]), main="Trace: beta[2]")
plot(ts(beta_mat[,3]), main="Trace: beta[3]")

effectiveSize(beta_mat[,1])
effectiveSize(beta_mat[,2])

```
```{r}
gamma_mat <- mh_test$samples$gamma  # n_keep x p

# pick a few components
par(mfrow=c(3,1))
plot(ts(gamma_mat[,1]), main="Trace: gamma[1]")
plot(ts(gamma_mat[,2]), main="Trace: gamma[2]")
plot(ts(gamma_mat[,3]), main="Trace: gamma[3]")

effectiveSize(gamma_mat[,1])
effectiveSize(gamma_mat[,2])

```

```{r}
library(parallel)
library(coda)

# -----------------------------
# Parallel multi-chain runner
# -----------------------------
run_mh_parallel <- function(
    n_chains = 4,
    Niter = 50000,
    burnin = 20000,
    thin = 10,
    target_accept = 0.20,
    adapt_every = 1000
) {

  # seeds for reproducibility
  seeds <- sample(1:9999999, n_chains)

  # What variables/functions need to be exported?
  needed_objects <- c(
    "run_mh",
    "init_par",  
    "X", "Z1", "Y1", "Y2",
    "log_post", "log_lik", "log_prior",
    "gev_logpdf_vec", "inv_logit",
    "u_to_xi_z1", "u_to_xi_y",
    "xi_z1_lower","xi_z1_upper","xi_y_lower","xi_y_upper",
    "cov_names", 
    "p", "N","propose_block"
  )

  # Start cluster
  cl <- makeCluster(n_chains)
  clusterExport(cl, needed_objects, envir = environment())

  # Wrapper to pass chain-specific seed
  chain_wrapper <- function(seed) {
    set.seed(seed)
    run_mh(
      Niter = Niter,
      burnin = burnin,
      thin = thin,
      target_accept = target_accept,
      adapt_every = adapt_every
    )
  }

  # Run chains in parallel
  results <- parLapply(cl, seeds, chain_wrapper)

  stopCluster(cl)

  return(results)
}


mh_chains <- run_mh_parallel(
  n_chains = 4,
  Niter = 100000,     # test values
  burnin = 20000,
  thin = 10
)

```
```{r}
alpha1_chains <- mcmc.list(
  mcmc(mh_chains[[1]]$samples$alpha[,1]),
  mcmc(mh_chains[[2]]$samples$alpha[,1]),
  mcmc(mh_chains[[3]]$samples$alpha[,1]),
  mcmc(mh_chains[[4]]$samples$alpha[,1])
)

gelman.diag(alpha1_chains)
effectiveSize(alpha1_chains)

```
```{r}
alpha1_chain_means <- sapply(mh_chains, function(ch) mean(ch$samples$alpha[,1]))
alpha1_chain_means

par(mfrow=c(2,2))
for (k in 1:4) {
  plot(ts(mh_chains[[k]]$samples$alpha[,1]),
       main = paste("Chain", k, "alpha[1]"))
}

```
```{r}
# beta[1]
beta1_chains <- mcmc.list(
  mcmc(mh_chains[[1]]$samples$beta[,1]),
  mcmc(mh_chains[[2]]$samples$beta[,1]),
  mcmc(mh_chains[[3]]$samples$beta[,1]),
  mcmc(mh_chains[[4]]$samples$beta[,1])
)
gelman.diag(beta1_chains)
effectiveSize(beta1_chains)

```
```{r}
beta1_chain_means <- sapply(mh_chains, function(ch) mean(ch$samples$beta[,2]))
beta1_chain_means

par(mfrow=c(2,2))
for (k in 1:4) {
  plot(ts(mh_chains[[k]]$samples$beta[,2]),
       main = paste("Chain", k, "beta[2]"))
}

```
```{r}
beta_mat <- mh_chains[[1]]$samples$beta  # n_keep x p

# pick a few components
par(mfrow=c(3,1))
plot(ts(beta_mat[,1]), main="Trace: alpha[1]")
plot(ts(beta_mat[,2]), main="Trace: alpha[2]")
plot(ts(beta_mat[,3]), main="Trace: alpha[3]")

effectiveSize(beta_mat[,1])
effectiveSize(beta_mat[,2])
effectiveSize(beta_mat[,3])
```
```{r}
gamma_mat <- mh_chains[[1]]$samples$gamma  # n_keep x p

# pick a few components
par(mfrow=c(2,2))
plot(ts(gamma_mat[,1]), main="Trace: gamma[1]")
plot(ts(gamma_mat[,2]), main="Trace: gamma[2]")
plot(ts(gamma_mat[,3]), main="Trace: gamma[3]")
plot(ts(gamma_mat[,4]), main="Trace: gamma[4]")

effectiveSize(gamma_mat[,1])
effectiveSize(gamma_mat[,2])
effectiveSize(gamma_mat[,3])
effectiveSize(gamma_mat[,4])
```
```{r}
for (i in 1:4) print(mh_chains[[i]]$acc_rates)
```

